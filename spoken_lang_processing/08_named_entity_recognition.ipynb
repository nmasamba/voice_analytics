{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the language model\n",
    "nlp_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_text = \"Hello there. I would like to order a taxi doctor from Musgrove Park Hospital in Zimbabwe. Ideally on 25 December please.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a spaCy doc\n",
    "doc = nlp_model(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello 0\n",
      "there 6\n",
      ". 11\n",
      "I 13\n",
      "would 15\n",
      "like 21\n",
      "to 26\n",
      "order 29\n",
      "a 35\n",
      "taxi 37\n",
      "doctor 42\n",
      "from 49\n",
      "Musgrove 54\n",
      "Park 63\n",
      "Hospital 68\n",
      "in 77\n",
      "Zimbabwe 80\n",
      ". 88\n",
      "Ideally 90\n",
      "on 98\n",
      "25 101\n",
      "December 104\n",
      "please 113\n",
      ". 119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inspect the tokens in the doc, along with their positions\n",
    "for token in doc:\n",
    "    print(token.text, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there.\n",
      "I would like to order a taxi doctor from Musgrove Park Hospital in Zimbabwe.\n",
      "Ideally on 25 December please.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inspect sentences in the doc\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musgrove Park Hospital ORG\n",
      "Zimbabwe GPE\n",
      "25 December DATE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# find named entities in the doc such as ORG, DATE, PRODUCT, TIME, PERSON, GPE\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fc81ddedec0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fc81ddedde0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fc81e0f5cd0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fc81de35870>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7fc81e1278c0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fc81e0f59d0>)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create custom named entities\n",
    "# you'll need to modify the Language class and associated pipeline; this is used by spaCy to determine rules for entities\n",
    "print(nlp_model.pipeline) # current pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fc81de17ec0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fc81ea23f30>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fc81ddf9950>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fc81f9d7c80>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7fc81f9e3050>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7fc821417b40>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fc81ea94bd0>)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# modifying the pipeline\n",
    "nlp_model2 = spacy.load('en_core_web_sm')\n",
    "custom_ruler = nlp_model2.add_pipe('entity_ruler', before='ner')\n",
    "patterns = [{'label':'SERVICE', 'pattern':'taxi'}, {'label':'DHOKOTA', 'pattern':'doctor'}]\n",
    "custom_ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "print(nlp_model2.pipeline) # new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi SERVICE\n",
      "doctor DHOKOTA\n",
      "Musgrove Park Hospital ORG\n",
      "Zimbabwe GPE\n",
      "25 December DATE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the new pipeline\n",
    "doc2 = nlp_model2(corpus_text)\n",
    "for entity in doc2.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
