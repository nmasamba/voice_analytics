{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02500564",
   "metadata": {},
   "source": [
    "Audio classification - just like with text - assigns a class label output from the input data. The only difference is instead of text inputs, you have raw audio waveforms. Some practical applications of audio classification include identifying speaker intent, language classification, and even animal species by their sounds.\n",
    "\n",
    "This guide shows how to:\n",
    "1. Finetune Wav2Vec2 on the MInDS-14 dataset to classify speaker intent.\n",
    "2. Use finetuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87450358",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c87235",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded22a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer, pipeline\n",
    "\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f98e9",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba955758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MInDS-14 dataset\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "\n",
    "# Split the dataset’s train split into a smaller train and test set\n",
    "# Chance to experiment and make sure everything works before spending more time on the full dataset\n",
    "minds = minds.train_test_split(test_size=0.2)\n",
    "\n",
    "# Inspect the data\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset contains a lot of useful information, like lang_id and english_transcription\n",
    "# We’ll focus on the audio and intent_class in this guide. Remove the other columns.\n",
    "minds = minds.remove_columns([\"path\", \"transcription\", \"english_transcription\", \"lang_id\"])\n",
    "\n",
    "# Two fields in the dataset:\n",
    "# audio: a 1-dimensional array of the speech signal that must be called to load and resample the audio file.\n",
    "# intent_class: represents the class id of the speaker’s intent.\n",
    "minds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea580ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map label name to label id\n",
    "labels = minds[\"train\"].features[\"intent_class\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "    \n",
    "# Now the label id can be converted to a label name\n",
    "id2label[str(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6082c1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f296ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a Wav2Vec2 feature extractor to process the audio signal\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MInDS-14 dataset has a sampling rate of 8000khz (you can find this information in it’s dataset card), \n",
    "# Need to resample the dataset to 16000kHz to use the pretrained Wav2Vec2 model\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "minds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94dd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a preprocessing function that:\n",
    "# Calls the audio column to load, and if necessary, resample the audio file\n",
    "# Checks if the sampling rate of audio file = sampling rate of the audio data a model was pretrained with\n",
    "# Set a maximum input length to batch longer inputs without truncating them\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62034b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_minds = minds.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_minds = encoded_minds.rename_column(\"intent_class\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2c27a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that passes your predictions and labels to compute to calculate the accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb96924",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wav2Vec2 with AutoModelForAudioClassification \n",
    "# Load the number of expected labels, and the label mappings\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training hyperparameters in TrainingArguments. \n",
    "# The only required parameter is output_dir which specifies where to save your model. \n",
    "# At end of each epoch, the Trainer will evaluate the accuracy and save the training checkpoint.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"audio_classification_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Pass the training arguments to Trainer \n",
    "# Also pass the model, dataset, tokenizer, data collator, and compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_minds[\"train\"],\n",
    "    eval_dataset=encoded_minds[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Call train() on the Trainer object to finetune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca827f7",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an audio file for inference\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "audio_file = dataset[0][\"audio\"][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f861879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using pipeline() object\n",
    "classifier = pipeline(\"audio-classification\", model=\"audio_classification_model\")\n",
    "classifier(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51013252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using PyTorch\n",
    "\n",
    "# Load a feature extractor to preprocess the audio file and return the input as PyTorch tensors\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"audio_classification_model\")\n",
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "# Pass inputs to the model and return the logits\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"audio_classification_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "# Get the class with the highest probability, and use the model’s id2label mapping to convert it to a label\n",
    "predicted_class_ids = torch.argmax(logits).item()\n",
    "predicted_label = model.config.id2label[predicted_class_ids]\n",
    "predicted_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
